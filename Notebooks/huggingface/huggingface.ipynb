{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HuggingFace\n",
    "Run this within the virtual environment **(env_ollama)**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/matthias/Desktop/MachineLearning/Ollama_Udemy/env_ollama/bin/python\n"
     ]
    }
   ],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matthias/Desktop/MachineLearning/Ollama_Udemy/env_ollama/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.9.1+cu128\n",
      "Device: cuda\n",
      "GPU: NVIDIA GeForce RTX 3060\n",
      "\n",
      "--- Generated text ---\n",
      "Once upon a time there was an old man, who had a daughter. They lived in the middle of nowhere, and the old man couldn't afford to send her to school, so he made his own books.\n",
      "His first book was about the moon. It was called Moon: The Story of Our Friend. He wrote it from memory, and read it aloud to his little girl every night.\n",
      "Then one day he started writing another story. This one was about a boy named John Smith, who loved horses very much. His horse's name was Jack. And when they were out riding together, they saw something strange on the horizon. A great white ship appeared, and they knew that they must hurry back home before their enemies found them.\n",
      "As they got closer, they saw more and more people crowded onto the deck, waving flags and shouting for joy.\n",
      "John and Jack were amazed at the sight. When they reached land again, they asked their father if he could tell them what this meant. But the old man said\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from dotenv import load_dotenv\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "load_dotenv()\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "#\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "if use_cuda:\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "#\n",
    "dtype = torch.float16 if use_cuda else torch.float32\n",
    "model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token=HF_TOKEN, use_fast=True)\n",
    "# use eos as pad token\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "# instantiate model and set device\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    token=HF_TOKEN,\n",
    "    #torch_dtype=dtype,\n",
    "    dtype=dtype,\n",
    "    device_map=\"auto\" if use_cuda else None\n",
    ")\n",
    "# ensure model is correct device\n",
    "if not use_cuda:\n",
    "    model.to(device)\n",
    "# switch model to evalutation\n",
    "model.eval()\n",
    "prompt = \"Once upon a time\"\n",
    "# get inputs\n",
    "inputs = tokenizer(\n",
    "    prompt,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=False,\n",
    "    truncation=True,\n",
    ")\n",
    "# move inputs to the right device (works for both CPU and GPU)\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "gen_kwargs = dict(\n",
    "    max_new_tokens=200,                 # generate 200 new tokens (recommended over max_length)\n",
    "    do_sample=True,                     # sampling (works with temperature/top_p)\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    repetition_penalty=1.1,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "# switch torch to inference mode\n",
    "with torch.inference_mode():\n",
    "    output_ids = model.generate(**inputs, **gen_kwargs)\n",
    "# prepare and print text\n",
    "text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "print(f\"\\n--- Generated text ---\\n{text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\checkmark$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
