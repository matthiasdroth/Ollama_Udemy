{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama PyTorch\n",
    "Run this within the virtual environment **(env_ollama)**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/matthias/Desktop/MachineLearning/Ollama_Udemy/env_ollama/bin/python\n"
     ]
    }
   ],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the `model`, `tokenizer`, and the `device`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matthias/Desktop/MachineLearning/Ollama_Udemy/env_ollama/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-15): 16 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "import torch\n",
    "from dotenv import load_dotenv\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "load_dotenv()\n",
    "warnings.filterwarnings(\"ignore\") # ignore all Python warnings\n",
    "HF_TOKEN = os.environ[\"HF_TOKEN\"]\n",
    "# Load the pre-trained LLaMA model and tokenizer (replace with the correct model name)\n",
    "model_name = \"meta-llama/Llama-3.2-1B\"  # Make sure this model exists on Hugging Face\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token=HF_TOKEN)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, token=HF_TOKEN)\n",
    "# Ensure pad_token_id is set (default to eos_token_id if not defined)\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model = model.to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put everything together and generate a text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time there was a 12-year-old boy who lived in a small village in the middle of nowhere. He was the son of a farmer and lived with his mother and two sisters. His father had died when he was very young, and his mother worked hard to provide for her family. The boy was a good student, and he loved to read and write. He was also a good athlete, and he enjoyed playing sports with his friends.\n",
      "One day, the boy was walking home from school when he saw a strange man standing on the side of the road. The man was wearing a long black coat and had a hood pulled over his head. The boy was scared and ran home as fast as he could. When he got home, he told his mother what he had seen, and she told him to go to the police.\n",
      "The boy went to the police station and told them what he had seen. The police took him to the station and questioned him about the man he had seen. The boy told them that the man was wearing a long black coat and had a hood pulled over his head. The police took him to the station and questioned him about the man he had seen. The boy told them that the man was wearing a long black coat and had a hood pulled over his head. The police took him to the station and questioned him about the man he had seen. The boy told them that the man was wearing a long black coat and had a hood pulled over\n"
     ]
    }
   ],
   "source": [
    "# Define a simple prompt\n",
    "prompt = \"Once upon a time there was a \"\n",
    "# Tokenize the input text and ensure the attention mask is included\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "# Get the attention mask from the tokenized inputs\n",
    "attention_mask = inputs.get(\"attention_mask\").to(device)\n",
    "# Move input tensors to the same device as the model\n",
    "input_ids = inputs[\"input_ids\"].to(device)\n",
    "# Generate output using the model, passing attention_mask\n",
    "with torch.no_grad():  # Disable gradient tracking for inference\n",
    "    output = model.generate(input_ids, max_length=300, num_beams=4, temperature=0.7, attention_mask=attention_mask,pad_token_id=tokenizer.pad_token_id)\n",
    "# Decode the generated tokens into text\n",
    "output_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "# Print the generated text\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\checkmark$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
