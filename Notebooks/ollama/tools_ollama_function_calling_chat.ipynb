{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7f14c88",
   "metadata": {},
   "source": [
    "# Multi‑turn Weather Chat\n",
    "Run this within the virtual environment **(env_ollama)**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f35e794a-0e4f-421c-b89d-59eb0d21f4d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/matthias/Desktop/MachineLearning/Ollama_Udemy/env_ollama/bin/python\n"
     ]
    }
   ],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50a2e77-8c11-4e89-bb35-dd7ddd655695",
   "metadata": {},
   "source": [
    "This notebook enables a **multi‑turn chat** about the **current weather** in multiple cities.\n",
    "\n",
    "- The script `open-meteo-api.py` can retrieve weather data from Open‑Meteo (but only for **Konstanz**, **Berlin** and **Munich**).\n",
    "- The notebook uses that `open-meteo-api.py` script **API** (`get_current_weather`) to fetch (almost) live weather for one of the predefined cities mentioned above.\n",
    "\n",
    "Requirements: `ollama serve` running locally, and a model like `llama3.2` pulled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a6c80f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-4.6°C at 2026-01-22 UTC\n",
      "Loaded. Cities: berlin (Berlin, DE), konstanz (Konstanz, DE), munich (Munich, DE)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import runpy\n",
    "import ollama\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "# This will execute the script once; we capture its globals and reuse the configured client.\n",
    "open_meteo_globals = runpy.run_path(str(Path.cwd() / \"../../Scripts/open-meteo-api.py\"))\n",
    "openmeteo = open_meteo_globals[\"openmeteo\"]\n",
    "# Define cities and their coordinates\n",
    "CITIES = {\n",
    "    \"berlin\":   {\"name\": \"Berlin, DE\",   \"lat\": 52.5200, \"lon\": 13.4050},\n",
    "    \"konstanz\": {\"name\": \"Konstanz, DE\", \"lat\": 47.6780, \"lon\": 9.1737},\n",
    "    \"munich\":   {\"name\": \"Munich, DE\",   \"lat\": 48.1374, \"lon\": 11.5755},\n",
    "}\n",
    "def list_cities() -> str:\n",
    "    return \", \".join([f\"{k} ({v['name']})\" for k, v in CITIES.items()])\n",
    "# Weather tool: current temperature etc. (Open‑Meteo, no API key) ---\n",
    "def get_current_weather(city: str) -> dict:\n",
    "    \"\"\"Get current weather for a predefined city key (berlin/konstanz/munich).\"\"\"\n",
    "    if not city:\n",
    "        raise ValueError(f\"Please specify a city. Options: {list_cities()}\")\n",
    "    key = city.strip().lower()\n",
    "    if key not in CITIES:\n",
    "        raise ValueError(f\"Unknown city '{city}'. Choose one of: {list_cities()}\")\n",
    "    loc = CITIES[key]\n",
    "    url = \"https://api.open-meteo.com/v1/forecast\"\n",
    "    params = {\n",
    "        \"latitude\": loc[\"lat\"],\n",
    "        \"longitude\": loc[\"lon\"],\n",
    "        # 'current' gives a snapshot around 'now'\n",
    "        \"current\": \",\".join([\n",
    "            \"temperature_2m\",\n",
    "            \"apparent_temperature\",\n",
    "            \"relative_humidity_2m\",\n",
    "            \"precipitation\",\n",
    "            \"wind_speed_10m\",\n",
    "        ]),\n",
    "        \"timezone\": \"UTC\",\n",
    "    }\n",
    "    response = openmeteo.weather_api(url, params=params)[0]\n",
    "    current = response.Current()\n",
    "    values = [current.Variables(i).Value() for i in range(current.VariablesLength())]\n",
    "    fields = [\"temperature_2m\", \"apparent_temperature\", \"relative_humidity_2m\", \"precipitation\", \"wind_speed_10m\"]\n",
    "    payload = {fields[i]: values[i] for i in range(len(fields))}\n",
    "    # Time is unix seconds in UTC\n",
    "    time_utc = pd.to_datetime(current.Time(), unit=\"s\", utc=True).isoformat()\n",
    "    # Return structured data\n",
    "    return {\n",
    "        \"city_key\": key,\n",
    "        \"city_name\": loc[\"name\"],\n",
    "        \"time_utc\": time_utc,\n",
    "        \"temperature_c\": payload[\"temperature_2m\"],\n",
    "        \"feels_like_c\": payload[\"apparent_temperature\"],\n",
    "        \"humidity_percent\": payload[\"relative_humidity_2m\"],\n",
    "        \"precip_mm\": payload[\"precipitation\"],\n",
    "        \"wind_kmh\": payload[\"wind_speed_10m\"],\n",
    "    }\n",
    "# Expose the tool to Ollama (function calling) ---\n",
    "TOOLS = [{\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"get_current_weather\",\n",
    "        \"description\": \"Get current weather for a predefined city key.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"city\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": f\"City key. One of: {', '.join(CITIES.keys())}. If user doesn't specify, ask them to choose.\",\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"city\"],\n",
    "        },\n",
    "    },\n",
    "}]\n",
    "def _run_tool_call(name: str, arguments: dict) -> str:\n",
    "    if name != \"get_current_weather\":\n",
    "        raise ValueError(f\"Unknown tool: {name}\")\n",
    "    return json.dumps(get_current_weather(**arguments), ensure_ascii=False)\n",
    "# Test loading the tool and listing cities\n",
    "print(\"Loaded. Cities:\", list_cities())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e30e8f",
   "metadata": {},
   "source": [
    "## Chat loop\n",
    "\n",
    "Run the cell below and chat. Examples:\n",
    "\n",
    "- “What’s the weather in Berlin right now?”\n",
    "- “Compare Konstanz and Munich.”\n",
    "- “Is it raining?” (it will ask which city)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddef8b25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weather chat ready. Type 'cities' to see options, 'quit' to exit.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  cities\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cities: berlin (Berlin, DE), konstanz (Konstanz, DE), munich (Munich, DE) \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  What is the weather like in Berlin?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: The current weather in Berlin is:\n",
      "\n",
      "* Temperature: 3.85°C\n",
      "* Feels like: 0.81°C\n",
      "* Humidity: 85%\n",
      "* Precipitation: 0 mm\n",
      "* Wind speed: 8 km/h\n",
      "\n",
      "Please note that these values are accurate as of the last update and may have changed since then.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  How warm is it Munich?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: The current weather in Munich is:\n",
      "\n",
      "* Temperature: 1.55°C\n",
      "* Feels like: -1.74°C\n",
      "* Humidity: 79%\n",
      "* Precipitation: 0 mm\n",
      "* Wind speed: 6 km/h\n",
      "\n",
      "Munich is quite chilly compared to Berlin, with a temperature that's almost 2°C lower.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  And how do Berlin and Munich compare to Konstanz?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: Here's a comparison of the current weather in Berlin, Munich, and Konstanz:\n",
      "\n",
      "* Temperature:\n",
      "\t+ Berlin: 3.85°C\n",
      "\t+ Munich: 1.55°C (almost 2°C lower than Berlin)\n",
      "\t+ Konstanz: 0.63°C (about 3.22°C lower than Berlin)\n",
      "* Feels like:\n",
      "\t+ Berlin: 0.81°C\n",
      "\t+ Munich: -1.74°C (very cold)\n",
      "\t+ Konstanz: -2.11°C (even colder than Munich)\n",
      "* Humidity:\n",
      "\t+ Berlin: 85%\n",
      "\t+ Munich: 79%\n",
      "\t+ Konstanz: 99% (very humid)\n",
      "* Precipitation:\n",
      "\t+ All three cities have 0 mm of precipitation\n",
      "* Wind speed:\n",
      "\t+ Berlin: 7.99 km/h\n",
      "\t+ Munich: 6.29 km/h\n",
      "\t+ Konstanz: 4.80 km/h (the calmest)\n",
      "\n",
      "Konstanz has the lowest temperature and feels like the coldest among the three cities, due to its high humidity.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  Thank you. Goodbye!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: It was nice assisting you with the weather information. Have a great day! Goodbye!\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  quit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bye!\n"
     ]
    }
   ],
   "source": [
    "def chat_weather(model: str = \"llama3.2\"):\n",
    "    system = (\n",
    "        \"You are a helpful weather assistant. \"\n",
    "        \"When you need live weather, call the get_current_weather tool. \"\n",
    "        f\"If the user doesn't specify a city, ask them to choose one of: {list_cities()}.\"\n",
    "    )\n",
    "    messages = [{\"role\": \"system\", \"content\": system}]\n",
    "    print(\"Weather chat ready. Type 'cities' to see options, 'quit' to exit.\\n\")\n",
    "    while True:\n",
    "        user = input(\"You: \").strip()\n",
    "        if not user:\n",
    "            continue\n",
    "        if user.lower() in {\"quit\", \"exit\"}:\n",
    "            print(\"Bye!\")\n",
    "            return\n",
    "        if user.lower() in {\"cities\", \"city\", \"locations\", \"locs\"}:\n",
    "            print(\"Cities:\", list_cities(), \"\\n\")\n",
    "            continue\n",
    "        messages.append({\"role\": \"user\", \"content\": user})\n",
    "        # First call: model may request tool usage\n",
    "        resp = ollama.chat(model=model, messages=messages, tools=TOOLS)\n",
    "        msg = resp.message\n",
    "        tool_calls = getattr(msg, \"tool_calls\", None)\n",
    "        if tool_calls:\n",
    "            # Add assistant message that contains tool calls\n",
    "            messages.append({\"role\": \"assistant\", \"content\": msg.content or \"\", \"tool_calls\": tool_calls})\n",
    "            # Execute each tool call and append tool results\n",
    "            for call in tool_calls:\n",
    "                fn = call.get(\"function\", {})\n",
    "                name = fn.get(\"name\")\n",
    "                args = fn.get(\"arguments\") or {}\n",
    "                if isinstance(args, str):\n",
    "                    args = json.loads(args)\n",
    "                tool_result = _run_tool_call(name, args)\n",
    "                messages.append({\"role\": \"tool\", \"name\": name, \"content\": tool_result})\n",
    "            # Second call: model produces final answer grounded in tool outputs\n",
    "            resp2 = ollama.chat(model=model, messages=messages)\n",
    "            answer = resp2.message.content\n",
    "            print(f\"Assistant: {answer}\\n\")\n",
    "            messages.append({\"role\": \"assistant\", \"content\": answer})\n",
    "        else:\n",
    "            answer = msg.content\n",
    "            print(f\"Assistant: {answer}\\n\")\n",
    "            messages.append({\"role\": \"assistant\", \"content\": answer})\n",
    "# Start:\n",
    "chat_weather(\"llama3.2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8ec4ae",
   "metadata": {},
   "source": [
    "Enter **quit** to exit the chat loop.\n",
    "\n",
    "$\\checkmark$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
